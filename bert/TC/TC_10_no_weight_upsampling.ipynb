{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Technical level - 10 classes with upsampling no weights\n",
    "Model: google-bert/bert-base-multilingual-uncased\n",
    "dev: 20%\n",
    "max_seq_length: 128\n",
    "train_batch_size: 8,\n",
    "eval_batch_size: 8,\n",
    "num_train_epochs: 5\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "242bb28e0578a4a5"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:39:28.172978400Z",
     "start_time": "2024-05-22T10:39:28.142983400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from tqdm import tqdm, tqdm_notebook, trange"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset, WeightedRandomSampler)\n",
    "from transformers import (BertConfig, AutoTokenizer, BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:39:36.246328200Z",
     "start_time": "2024-05-22T10:39:29.134571100Z"
    }
   },
   "id": "9e354f6c4cd92628",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "182ba2744f21a010"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_articles = \"datasets/upsampling/train\"\n",
    "dev_articles = \"datasets/dev\"\n",
    "train_TC_labels = \"datasets/upsampling/train_TC_labels\"\n",
    "dev_TC_template = \"datasets/TC_labels_for_eval.txt\"  \n",
    "true_file = \"datasets/TC.labels_true.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:39:36.267339400Z",
     "start_time": "2024-05-22T10:39:36.250332200Z"
    }
   },
   "id": "eff69aea807dc5cf",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "techniques = \"datasets/propaganda-techniques-names.txt\"\n",
    "PROP_TECH_TO_LABEL = {}  #to dictionary\n",
    "LABEL_TO_PROP_TECH = {}     #to list\n",
    "label = 0\n",
    "with open(techniques, \"r\") as f:\n",
    "  for technique in f:\n",
    "    PROP_TECH_TO_LABEL[technique.replace(\"\\n\", \"\")] = int(label)\n",
    "    LABEL_TO_PROP_TECH[int(label)] = technique.replace(\"\\n\", \"\")\n",
    "    label += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:39:36.301327300Z",
     "start_time": "2024-05-22T10:39:36.284328600Z"
    }
   },
   "id": "1b1cc9cc7cca1fcc",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "n_gpu = torch.cuda.device_count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:39:58.431982500Z",
     "start_time": "2024-05-22T10:39:58.405937600Z"
    }
   },
   "id": "f7ac9c13fd8fba18",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\"bert\": (BertConfig, BertForSequenceClassification, BertTokenizer)}  \n",
    "args = {\"data_dir\": \"bert/\",  \n",
    "        \"model_type\": \"bert\", \n",
    "        \"model_name\": \"google-bert/bert-base-multilingual-uncased\",\n",
    "        \"output_dir\": \"output_dir/output_TC_10_no_weight_up\",\n",
    "        \"max_seq_length\": 128, \n",
    "        \"train_batch_size\": 8,\n",
    "        \"eval_batch_size\": 8,\n",
    "        \"num_train_epochs\": 5, \n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 4e-5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"gradient_accumulation_steps\": 1,  \n",
    "        \"save_steps\": 2000,\n",
    "        \"overwrite_output_dir\": False}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:40:16.181447300Z",
     "start_time": "2024-05-22T10:40:16.153040Z"
    }
   },
   "id": "a4ffffe8b2ecd8f2",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bece42e74b332b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_dataframe_to_features(dataframe, max_seq_length, tokenizer):\n",
    "  \"\"\"\n",
    "  Converts dataframe into features dataframe, where each feature will\n",
    "  take form of [CLS] + A + [SEP]\n",
    "  \"\"\"\n",
    "  # Create features\n",
    "  features = pd.DataFrame(None, range(dataframe.shape[0]), \n",
    "                              [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_ids\"])\n",
    "\n",
    "  # For each sequence, do:\n",
    "  for i in range(len(dataframe)):\n",
    "    # Set first and second part of the sequences\n",
    "    tokens = tokenizer.tokenize(dataframe[\"text\"][i])\n",
    "\n",
    "    # If length of the sequence is greater than max sequence length, truncate it\n",
    "    if len(tokens) > max_seq_length - 2:\n",
    "        tokens = tokens[:(max_seq_length - 2)]\n",
    "\n",
    "    # Concatenate the tokens\n",
    "    tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
    "\n",
    "    # Compute the ids\n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    pad_token = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "    input_ids = input_ids + [pad_token] * padding_length\n",
    "    input_mask = input_mask + [0] * padding_length\n",
    "    segment_ids = segment_ids + [0] * padding_length\n",
    "    label_id = dataframe[\"label\"][i]\n",
    "\n",
    "    # Assert to make sure we have same length\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    # Put the data into features dataframe\n",
    "    features[\"input_ids\"][i] = input_ids\n",
    "    features[\"input_mask\"][i] = input_mask\n",
    "    features[\"segment_ids\"][i] = segment_ids\n",
    "    features[\"label_ids\"][i] = int(label_id)    \n",
    "    \n",
    "\n",
    "  return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:40:17.918324200Z",
     "start_time": "2024-05-22T10:40:17.904321400Z"
    }
   },
   "id": "6e0efefc17e99462",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def articles_to_dataframe(article_folder, label_folder):\n",
    "  \"\"\"\n",
    "  Preprocesses the articles into dataframes with sequences with binary tags\n",
    "  \"\"\"\n",
    "  # First sort the filenames and make sure we have label file for each articles\n",
    "  article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "  label_filenames = sorted(glob.glob(os.path.join(label_folder, \"*.labels.tsv\")))\n",
    "  assert len(article_filenames) == len(label_filenames)\n",
    "\n",
    "  # Initialize sequences\n",
    "  sequences = []\n",
    "\n",
    "  # For each article, do:\n",
    "  for i in range(len(article_filenames)):\n",
    "    # Read in the article\n",
    "    with codecs.open(article_filenames[i], \"r\", encoding=\"utf8\") as f:\n",
    "      article = f.read()\n",
    "\n",
    "    # Read in the label file and store indices \n",
    "    with open(label_filenames[i], \"r\") as f:\n",
    "      reader = csv.reader(f, delimiter=\"\\t\")\n",
    "      article_sequences = []\n",
    "      labels_list = []\n",
    "      for row in reader:\n",
    "        article_sequences.append(article[int(row[2]):int(row[3])])\n",
    "        labels_list.append(PROP_TECH_TO_LABEL[row[1]])\n",
    "\n",
    "    sequence = pd.DataFrame(None, range(len(article_sequences)), [\"label\", \"text\"])\n",
    "    sequence[\"label\"] = labels_list\n",
    "    sequence[\"text\"] = article_sequences   \n",
    "\n",
    "    # Add to the sequences\n",
    "    sequences.append(sequence)\n",
    "\n",
    "  # Concatenate all dataframes\n",
    "  dataframe = pd.concat(sequences, ignore_index=True)\n",
    "\n",
    "  return dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:40:18.874381300Z",
     "start_time": "2024-05-22T10:40:18.857324800Z"
    }
   },
   "id": "aa0b5ec20e2474b6",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_training_dataset_from_articles(articles_folders, labels_folders, tokenizer):\n",
    "  \"\"\"\n",
    "  Generates dataset to go into BERT from articles and labels\n",
    "  \"\"\"\n",
    "  \n",
    "  # For each articles and labels folder set, turn them into dataframes\n",
    "  dataframe_list = []\n",
    "  for i in range(len(articles_folders)):\n",
    "    dataframe_list.append(articles_to_dataframe(articles_folders[i], labels_folders[i]))\n",
    "\n",
    "  # Concatenate the dataframes to make a total dataframe\n",
    "  dataframe = pd.concat(dataframe_list, ignore_index=True)\n",
    "  # Process into features dataframe\n",
    "  features = convert_dataframe_to_features(dataframe, args['max_seq_length'], tokenizer) #\n",
    "     \n",
    "  # Creating TensorDataset from features\n",
    "  all_input_ids = torch.tensor(features[\"input_ids\"], dtype=torch.long)\n",
    "  all_input_mask = torch.tensor(features[\"input_mask\"], dtype=torch.long)\n",
    "  all_segment_ids = torch.tensor(features[\"segment_ids\"], dtype=torch.long)\n",
    "  all_label_ids = torch.tensor(features[\"label_ids\"], dtype=torch.long)\n",
    "\n",
    "  dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "  \n",
    "  return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:40:20.066376700Z",
     "start_time": "2024-05-22T10:40:20.044340400Z"
    }
   },
   "id": "4f8387dcea3489f9",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1b86f90a06d6d1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Without weights\n",
    "def train_no_weights (train_dataset, model):\n",
    "    \"\"\"\n",
    "    Trains the model with training dataset\n",
    "    \"\"\"\n",
    "    # Initialize various necessary objects\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size']) \n",
    "    \n",
    "    # Compute the total time\n",
    "    t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "    \n",
    "    # Set the grouped parameters for optimizer\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    # Compute warmup steps\n",
    "    warmup_steps = math.ceil(t_total * args['warmup_ratio'])\n",
    "    args['warmup_steps'] = warmup_steps if args['warmup_steps'] == 0 else args['warmup_steps']\n",
    "    \n",
    "    # Initialize optimizer as Adam with constant weight decay and a linear scheduler with warmup\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['warmup_steps'], num_training_steps=t_total)\n",
    "\n",
    "    # Initialize variables for training\n",
    "    global_step = 0\n",
    "    tr_loss = 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "    \n",
    "    # Start training\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2], \n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "                \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step() \n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "\n",
    "                if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
    "                    output_dir = os.path.join(args['output_dir'], 'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T11:16:17.631140100Z",
     "start_time": "2024-05-22T11:16:17.615142100Z"
    }
   },
   "id": "5703f32dfb8a8528",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=10, bias=True)\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args[\"model_type\"]]\n",
    "config = config_class.from_pretrained(args[\"model_name\"], num_labels=len(PROP_TECH_TO_LABEL))\n",
    "tokenizer = tokenizer_class.from_pretrained(args[\"model_name\"])\n",
    "model = model_class(config)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T11:16:24.959139400Z",
     "start_time": "2024-05-22T11:16:21.355140200Z"
    }
   },
   "id": "709c24dabb588a99",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = generate_training_dataset_from_articles([train_articles], [train_TC_labels], tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:41:00.758891600Z",
     "start_time": "2024-05-22T10:40:30.120755300Z"
    }
   },
   "id": "a79a196de9f4a0cc",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Windows\\Temp\\ipykernel_17816\\1938281005.py:36: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1986c16f7e624350ad0e53aa5700a979"
      },
      "application/json": {
       "n": 0,
       "total": 556,
       "elapsed": 0.003998279571533203,
       "ncols": null,
       "nrows": null,
       "prefix": "Iteration",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 1/5 [33:21<2:13:24, 2001.21s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "303d451eea254b0b8c28a9426ac404d3"
      },
      "application/json": {
       "n": 0,
       "total": 556,
       "elapsed": 0.030989646911621094,
       "ncols": null,
       "nrows": null,
       "prefix": "Iteration",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 2/5 [1:09:29<1:44:58, 2099.62s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a581f6d398449cd891adf37bd014bf7"
      },
      "application/json": {
       "n": 0,
       "total": 556,
       "elapsed": 0.016984939575195312,
       "ncols": null,
       "nrows": null,
       "prefix": "Iteration",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 3/5 [1:42:35<1:08:15, 2047.59s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "299542c7f9994b788a30901e7b32fe0a"
      },
      "application/json": {
       "n": 0,
       "total": 556,
       "elapsed": 0.01237630844116211,
       "ncols": null,
       "nrows": null,
       "prefix": "Iteration",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 4/5 [2:16:57<34:13, 2053.41s/it]  "
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b6e9b56fa03474fac64ca82dce5da1c"
      },
      "application/json": {
       "n": 0,
       "total": 556,
       "elapsed": 0.009667634963989258,
       "ncols": null,
       "nrows": null,
       "prefix": "Iteration",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [2:56:19<00:00, 2115.86s/it]\n"
     ]
    }
   ],
   "source": [
    "global_step, tr_loss = train_no_weights(train_dataset, model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T14:12:56.998848500Z",
     "start_time": "2024-05-22T11:16:37.617028100Z"
    }
   },
   "id": "8f51f3a1c2a68913",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(args['output_dir'])\n",
    "tokenizer.save_pretrained(args['output_dir'])\n",
    "torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T14:14:49.673372300Z",
     "start_time": "2024-05-22T14:14:48.224761200Z"
    }
   },
   "id": "1dfc3a0da2c1c207",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d024aa4ad299f1ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_TC_eval_dataset_from_article(article_folder, indices_file, tokenizer):\n",
    "  \"\"\"\n",
    "  Generates TC dataset to go into BERT from articles and labels\n",
    "  \"\"\"\n",
    "  # First sort the filenames and make sure we have label file for each articles\n",
    "  article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "  articles = {}\n",
    "\n",
    "  # For each article, read them in:\n",
    "  for i in range(len(article_filenames)):\n",
    "    article_id = os.path.basename(article_filenames[i]).split(\".\")[0][7:]\n",
    "    with codecs.open(article_filenames[i], \"r\", encoding=\"utf8\") as f:\n",
    "      articles[article_id] = f.read()\n",
    "\n",
    "  # Read in indices file\n",
    "  with open(indices_file, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    ids_list = []\n",
    "    seq_starts = []\n",
    "    seq_ends = []\n",
    "    article_sequences = []\n",
    "    for row in reader:\n",
    "      ids_list.append(row[0])\n",
    "      seq_starts.append(row[2])\n",
    "      seq_ends.append(row[3])\n",
    "      article_sequences.append(articles[row[0]][int(row[2]):int(row[3])])\n",
    "\n",
    "  dataframe = pd.DataFrame(None, range(len(ids_list)), [\"id\", \"seq_starts\", \"seq_ends\", \"label\", \"text\"])\n",
    "  dataframe[\"id\"] = ids_list\n",
    "  dataframe[\"seq_starts\"] = seq_starts\n",
    "  dataframe[\"seq_ends\"] = seq_ends\n",
    "  dataframe[\"label\"] = [0] * len(ids_list)\n",
    "  dataframe[\"text\"] = article_sequences\n",
    "\n",
    "  # Process into features dataframe\n",
    "  features = convert_dataframe_to_features(dataframe, args['max_seq_length'], tokenizer)\n",
    "      \n",
    "  # Creating TensorDataset from features\n",
    "  all_input_ids = torch.tensor(features[\"input_ids\"], dtype=torch.long)\n",
    "  all_input_mask = torch.tensor(features[\"input_mask\"], dtype=torch.long)\n",
    "  all_segment_ids = torch.tensor(features[\"segment_ids\"], dtype=torch.long)\n",
    "  all_label_ids = torch.tensor(features[\"label_ids\"], dtype=torch.long)\n",
    "\n",
    "  dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "  return dataset, dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T00:35:14.030755900Z",
     "start_time": "2024-05-17T00:35:13.968725800Z"
    }
   },
   "id": "ba2e7fc6aa760b8",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def classify_techniques(eval_dataset, model):\n",
    "  \"\"\"\n",
    "  Classifies a single article dataset and returns article id with indices list\n",
    "  \"\"\"\n",
    "  # Load the eval data and initialize sampler\n",
    "  eval_sampler = SequentialSampler(eval_dataset)\n",
    "  eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "  # Start Classification\n",
    "  preds = None\n",
    "\n",
    "  # For each batch, evaluate\n",
    "  for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      inputs = {'input_ids':      batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'token_type_ids': batch[2],\n",
    "                'labels':         batch[3]}\n",
    "      outputs = model(**inputs)\n",
    "      logits = outputs[1]\n",
    "\n",
    "    # Get predictions\n",
    "    if preds is None:\n",
    "      preds = logits.detach().cpu().numpy()\n",
    "    else:\n",
    "      preds = numpy.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "  # Get the most probable prediction\n",
    "  preds = numpy.argmax(preds, axis=1)\n",
    "\n",
    "  return preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T00:35:14.078712300Z",
     "start_time": "2024-05-17T00:35:13.998708400Z"
    }
   },
   "id": "2a9362a68934816f",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOG:Generating evaluation dataset...\n",
      "INFO:LOG:Creating features from dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id seq_starts seq_ends  label  \\\n",
      "0      209813          0        3      0   \n",
      "1      209813         19       36      0   \n",
      "2      209813         49       65      0   \n",
      "3      209813        235      249      0   \n",
      "4      210028          0        6      0   \n",
      "...       ...        ...      ...    ...   \n",
      "1027  1633033         68       80      0   \n",
      "1028  1636639        325      376      0   \n",
      "1029  1636685        879      885      0   \n",
      "1030  1636685        968     1051      0   \n",
      "1031  1636685       1183     1219      0   \n",
      "\n",
      "                                                   text  \n",
      "0                                                   ска  \n",
      "1                                     продовжує ширитис  \n",
      "2                                      крейзі-пухнастик  \n",
      "3                                        головний прико  \n",
      "4                                                виродк  \n",
      "...                                                 ...  \n",
      "1027                                       проживає в р  \n",
      "1028  і це бля в той час, коли на фронті дефіцит сна...  \n",
      "1029                                             гостри  \n",
      "1030  але чи означає це, що на деякий час українсько...  \n",
      "1031               як це доводилося чути києву у вересн  \n",
      "\n",
      "[1032 rows x 5 columns]\n",
      "(1032, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOG:Creating TensorDataset from features dataframe\n",
      "INFO:LOG:***** Running classification for article 209813 *****\n",
      "INFO:LOG:  Num sequences = 1032\n",
      "INFO:LOG:  Batch size = 8\n",
      "C:\\Windows\\Temp\\ipykernel_21804\\3392312430.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n"
     ]
    },
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/129 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bd140aba12e4bb1b3ce759088d071f3"
      },
      "application/json": {
       "n": 0,
       "total": 129,
       "elapsed": 0.008994102478027344,
       "ncols": null,
       "nrows": null,
       "prefix": "Evaluating",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "article_filenames = sorted(glob.glob(os.path.join(dev_articles, \"*.txt\")))\n",
    "\n",
    "output_file = 'datasets/upsampling/TC.labels_pred_10_no_weight_up.txt'\n",
    "f = open(output_file, 'w', newline='')\n",
    "writer = csv.writer(f, delimiter='\\t')\n",
    "eval_dataset, eval_dataframe = generate_TC_eval_dataset_from_article(dev_articles, dev_TC_template, tokenizer)\n",
    "predictions = classify_techniques(eval_dataset, model)\n",
    "for i in range(len(predictions)):\n",
    "  writer.writerow([eval_dataframe[\"id\"][i], LABEL_TO_PROP_TECH[predictions[i]], eval_dataframe[\"seq_starts\"][i], eval_dataframe[\"seq_ends\"][i]])\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T00:35:54.340522200Z",
     "start_time": "2024-05-17T00:35:14.016751200Z"
    }
   },
   "id": "8b265cbb9e69aa99",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### References:\n",
    "The structure of data preporation is the modification of Henry Kim's [implementation](https://medium.com/@jihwangk/fine-grained-propaganda-detection-and-classification-with-bert-dfad4acaa321).\n",
    "\n",
    "The structure of train() function is a modification of Thilina Rajapakse’s [implementation](https://github.com/ThilinaRajapakse/pytorch-transformers-classification)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e943640e71c68501"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cdbe9b3f50d8c50c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
