{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Technical level 10 classes no weight\n",
    "Model: google-bert/bert-base-multilingual-uncased\n",
    "max_seq_length: 128\n",
    "train_batch_size: 8,\n",
    "eval_batch_size: 8,\n",
    "num_train_epochs: 5\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "242bb28e0578a4a5"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:36:40.888104300Z",
     "start_time": "2024-05-22T10:36:40.870104400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from tqdm import tqdm, tqdm_notebook, trange"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from transformers import (BertConfig, AutoTokenizer, BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:36:41.331103900Z",
     "start_time": "2024-05-22T10:36:41.311107500Z"
    }
   },
   "id": "9e354f6c4cd92628",
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "182ba2744f21a010"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#PATH TO DATA \n",
    "train_articles = \"datasets/train\"\n",
    "dev_articles = \"datasets/dev\"\n",
    "train_TC_labels = \"datasets/train_TC_labels\"\n",
    "dev_TC_template = \"datasets/TC_labels_for_eval.txt\"  \n",
    "true_file = \"datasets/TC.labels_true.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:36:47.137146700Z",
     "start_time": "2024-05-22T10:36:47.114154900Z"
    }
   },
   "id": "5951bb2f432926bc",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#techniques = \"datasets/propaganda-techniques-names.txt\"\n",
    "techniques = \"datasets/propaganda-techniques-names.txt\"\n",
    "PROP_TECH_TO_LABEL = {}  #to dictionary\n",
    "LABEL_TO_PROP_TECH = {}     #to list\n",
    "label = 0\n",
    "with open(techniques, \"r\") as f:\n",
    "  for technique in f:\n",
    "    PROP_TECH_TO_LABEL[technique.replace(\"\\n\", \"\")] = int(label)\n",
    "    LABEL_TO_PROP_TECH[int(label)] = technique.replace(\"\\n\", \"\")\n",
    "    label += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:36:52.203212Z",
     "start_time": "2024-05-22T10:36:52.176217500Z"
    }
   },
   "id": "1b1cc9cc7cca1fcc",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "n_gpu = torch.cuda.device_count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:36:52.636232200Z",
     "start_time": "2024-05-22T10:36:52.613234900Z"
    }
   },
   "id": "f7ac9c13fd8fba18",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\"bert\": (BertConfig, BertForSequenceClassification, BertTokenizer)}  \n",
    "args = {\"data_dir\": \"bert/\",  \n",
    "        \"model_type\": \"bert\", \n",
    "        \"model_name\": \"google-bert/bert-base-multilingual-uncased\",\n",
    "        \"output_dir\": \"output_dir/output_TC_10_no_weight\",  \n",
    "        \"max_seq_length\": 128,  \n",
    "        \"train_batch_size\": 8,\n",
    "        \"eval_batch_size\": 8,\n",
    "        \"num_train_epochs\": 5, \n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 4e-5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"save_steps\": 2000,\n",
    "        \"overwrite_output_dir\": False}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:36:53.192233Z",
     "start_time": "2024-05-22T10:36:53.183233700Z"
    }
   },
   "id": "a4ffffe8b2ecd8f2",
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bece42e74b332b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_dataframe_to_features(dataframe, max_seq_length, tokenizer):\n",
    "  \"\"\"\n",
    "  Converts dataframe into features dataframe, where each feature will\n",
    "  take form of [CLS] + A + [SEP]\n",
    "  \"\"\"\n",
    "  # Create features\n",
    "  features = pd.DataFrame(None, range(dataframe.shape[0]), \n",
    "                              [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_ids\"])\n",
    "\n",
    "  # For each sequence, do:\n",
    "  for i in range(len(dataframe)):\n",
    "    # Set first and second part of the sequences\n",
    "    tokens = tokenizer.tokenize(dataframe[\"text\"][i])\n",
    "\n",
    "    # If length of the sequence is greater than max sequence length, truncate it\n",
    "    if len(tokens) > max_seq_length - 2:\n",
    "        tokens = tokens[:(max_seq_length - 2)]\n",
    "\n",
    "    # Concatenate the tokens\n",
    "    tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
    "\n",
    "    # Compute the ids\n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    pad_token = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "    input_ids = input_ids + [pad_token] * padding_length\n",
    "    input_mask = input_mask + [0] * padding_length\n",
    "    segment_ids = segment_ids + [0] * padding_length\n",
    "    label_id = dataframe[\"label\"][i]\n",
    "\n",
    "    # Assert to make sure we have same length\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    # Put the data into features dataframe\n",
    "    features[\"input_ids\"][i] = input_ids\n",
    "    features[\"input_mask\"][i] = input_mask\n",
    "    features[\"segment_ids\"][i] = segment_ids\n",
    "    features[\"label_ids\"][i] = int(label_id)  \n",
    "    \n",
    "\n",
    "  return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:36:55.430493500Z",
     "start_time": "2024-05-22T10:36:55.421476300Z"
    }
   },
   "id": "6e0efefc17e99462",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def articles_to_dataframe(article_folder, label_folder):\n",
    "  \"\"\"\n",
    "  Preprocesses the articles into dataframes with sequences with binary tags\n",
    "  \"\"\"\n",
    "  # First sort the filenames and make sure we have label file for each articles\n",
    "  article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "  label_filenames = sorted(glob.glob(os.path.join(label_folder, \"*.labels.tsv\")))\n",
    "  assert len(article_filenames) == len(label_filenames)\n",
    "\n",
    "  # Initialize sequences\n",
    "  sequences = []\n",
    "\n",
    "  # For each article, do:\n",
    "  for i in range(len(article_filenames)):\n",
    "\n",
    "    # Read in the article\n",
    "    with codecs.open(article_filenames[i], \"r\", encoding=\"utf8\") as f:\n",
    "      article = f.read()\n",
    "\n",
    "    # Read in the label file and store indices \n",
    "    with open(label_filenames[i], \"r\") as f:\n",
    "      reader = csv.reader(f, delimiter=\"\\t\")\n",
    "      article_sequences = []\n",
    "      labels_list = []\n",
    "      for row in reader:\n",
    "        article_sequences.append(article[int(row[2]):int(row[3])])\n",
    "        labels_list.append(PROP_TECH_TO_LABEL[row[1]])\n",
    "\n",
    "    sequence = pd.DataFrame(None, range(len(article_sequences)), [\"label\", \"text\"])\n",
    "    sequence[\"label\"] = labels_list\n",
    "    sequence[\"text\"] = article_sequences   \n",
    "\n",
    "    # Add to the sequences\n",
    "    sequences.append(sequence)\n",
    "\n",
    "  # Concatenate all dataframes\n",
    "  dataframe = pd.concat(sequences, ignore_index=True)\n",
    "\n",
    "  return dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:36:56.021918400Z",
     "start_time": "2024-05-22T10:36:55.993917300Z"
    }
   },
   "id": "aa0b5ec20e2474b6",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_training_dataset_from_articles(articles_folders, labels_folders, tokenizer):\n",
    "  \"\"\"\n",
    "  Generates dataset to go into BERT from articles and labels\n",
    "  \"\"\"\n",
    "    \n",
    "  # For each articles and labels folder set, turn them into dataframes\n",
    "  dataframe_list = []\n",
    "  for i in range(len(articles_folders)):\n",
    "    dataframe_list.append(articles_to_dataframe(articles_folders[i], labels_folders[i]))\n",
    "\n",
    "  # Concatenate the dataframes to make a total dataframe\n",
    "  dataframe = pd.concat(dataframe_list, ignore_index=True)\n",
    "\n",
    "  # Process into features dataframe\n",
    "  features = convert_dataframe_to_features(dataframe, args['max_seq_length'], tokenizer) \n",
    "     \n",
    "  # Creating TensorDataset from features\n",
    "  all_input_ids = torch.tensor(features[\"input_ids\"], dtype=torch.long)\n",
    "  all_input_mask = torch.tensor(features[\"input_mask\"], dtype=torch.long)\n",
    "  all_segment_ids = torch.tensor(features[\"segment_ids\"], dtype=torch.long)\n",
    "  all_label_ids = torch.tensor(features[\"label_ids\"], dtype=torch.long)\n",
    "\n",
    "  dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "  \n",
    "  return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:36:56.443954100Z",
     "start_time": "2024-05-22T10:36:56.433954200Z"
    }
   },
   "id": "4f8387dcea3489f9",
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1b86f90a06d6d1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# No weights\n",
    "def train(train_dataset, model):\n",
    "  \"\"\"\n",
    "  Trains the model with training dataset\n",
    "  \"\"\"\n",
    "  # Initialize various necessary objects\n",
    "  train_sampler = RandomSampler(train_dataset)\n",
    "  train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size']) \n",
    "    \n",
    "  # Compute the total time\n",
    "  t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "  \n",
    "  # Set the grouped parameters for optimizer\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\n",
    "  optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "  \n",
    "  # Compute warmup steps\n",
    "  warmup_steps = math.ceil(t_total * args['warmup_ratio'])\n",
    "  args['warmup_steps'] = warmup_steps if args['warmup_steps'] == 0 else args['warmup_steps']\n",
    "  \n",
    "  # Initialize optimizer as Adam with constant weight decay and a linear scheduler with warmup\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['warmup_steps'], num_training_steps=t_total)\n",
    "\n",
    "  # Initialize variables for training\n",
    "  global_step = 0\n",
    "  tr_loss  = 0.0\n",
    "  model.zero_grad()\n",
    "  train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "  \n",
    "  # Start training\n",
    "  for _ in train_iterator:\n",
    "    epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
    "    #epoch_iterator = tqdm.notebook.tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "      model.train()\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      inputs = {'input_ids':      batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'token_type_ids': batch[2], \n",
    "                'labels':         batch[3]}\n",
    "      outputs = model(**inputs)\n",
    "      loss = outputs[0]\n",
    "\n",
    "      if args['gradient_accumulation_steps'] > 1:\n",
    "        loss = loss / args['gradient_accumulation_steps']\n",
    "          \n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "\n",
    "      tr_loss += loss.item()\n",
    "      if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "        model.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
    "          output_dir = os.path.join(args['output_dir'], 'checkpoint-{}'.format(global_step))\n",
    "          if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "          model_to_save = model.module if hasattr(model, 'module') else model\n",
    "          model_to_save.save_pretrained(output_dir)\n",
    "\n",
    "  return global_step, tr_loss / global_step"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:36:57.706872500Z",
     "start_time": "2024-05-22T10:36:57.682874900Z"
    }
   },
   "id": "ccac878d44301c99",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=10, bias=True)\n)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args[\"model_type\"]]\n",
    "config = config_class.from_pretrained(args[\"model_name\"], num_labels=len(PROP_TECH_TO_LABEL))\n",
    "tokenizer = tokenizer_class.from_pretrained(args[\"model_name\"])\n",
    "model = model_class(config)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:37:02.654265200Z",
     "start_time": "2024-05-22T10:36:59.089769200Z"
    }
   },
   "id": "709c24dabb588a99",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = generate_training_dataset_from_articles([train_articles], [train_TC_labels], tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:37:16.025293400Z",
     "start_time": "2024-05-22T10:37:04.037778100Z"
    }
   },
   "id": "a79a196de9f4a0cc",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "global_step, tr_loss = train(train_dataset, model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:38:21.442907400Z",
     "start_time": "2024-05-22T10:38:21.421907300Z"
    }
   },
   "id": "8f51f3a1c2a68913",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(args['output_dir'])\n",
    "tokenizer.save_pretrained(args['output_dir'])\n",
    "torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:16:32.004957800Z",
     "start_time": "2024-05-22T10:16:30.294572400Z"
    }
   },
   "id": "1dfc3a0da2c1c207",
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d024aa4ad299f1ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_TC_eval_dataset_from_article(article_folder, indices_file, tokenizer):\n",
    "  \"\"\"\n",
    "  Generates TC dataset to go into BERT from articles and labels\n",
    "  \"\"\"\n",
    "\n",
    "  # First sort the filenames and make sure we have label file for each articles\n",
    "  article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "  articles = {}\n",
    "\n",
    "  # For each article, read them in:\n",
    "  for i in range(len(article_filenames)):\n",
    "    article_id = os.path.basename(article_filenames[i]).split(\".\")[0][7:]\n",
    "    with codecs.open(article_filenames[i], \"r\", encoding=\"utf8\") as f:\n",
    "      articles[article_id] = f.read()\n",
    "\n",
    "  # Read in indices file\n",
    "  with open(indices_file, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    ids_list = []\n",
    "    seq_starts = []\n",
    "    seq_ends = []\n",
    "    article_sequences = []\n",
    "    for row in reader:\n",
    "      ids_list.append(row[0])\n",
    "      seq_starts.append(row[2])\n",
    "      seq_ends.append(row[3])\n",
    "      article_sequences.append(articles[row[0]][int(row[2]):int(row[3])])\n",
    "\n",
    "  dataframe = pd.DataFrame(None, range(len(ids_list)), [\"id\", \"seq_starts\", \"seq_ends\", \"label\", \"text\"])\n",
    "  dataframe[\"id\"] = ids_list\n",
    "  dataframe[\"seq_starts\"] = seq_starts\n",
    "  dataframe[\"seq_ends\"] = seq_ends\n",
    "  dataframe[\"label\"] = [0] * len(ids_list)\n",
    "  dataframe[\"text\"] = article_sequences\n",
    "\n",
    "  # Process into features dataframe\n",
    "  features = convert_dataframe_to_features(dataframe, args['max_seq_length'], tokenizer)\n",
    "      \n",
    "  # Creating TensorDataset from features\n",
    "  all_input_ids = torch.tensor(features[\"input_ids\"], dtype=torch.long)\n",
    "  all_input_mask = torch.tensor(features[\"input_mask\"], dtype=torch.long)\n",
    "  all_segment_ids = torch.tensor(features[\"segment_ids\"], dtype=torch.long)\n",
    "  all_label_ids = torch.tensor(features[\"label_ids\"], dtype=torch.long)\n",
    "\n",
    "  dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "  return dataset, dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:16:36.006184900Z",
     "start_time": "2024-05-22T10:16:35.989185300Z"
    }
   },
   "id": "ba2e7fc6aa760b8",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def classify_techniques(eval_dataset, model):\n",
    "  \"\"\"\n",
    "  Classifies a single article dataset and returns article id with indices list\n",
    "  \"\"\"\n",
    "  # Load the eval data and initialize sampler\n",
    "  eval_sampler = SequentialSampler(eval_dataset)\n",
    "  eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "  preds = None\n",
    "\n",
    "  # For each batch, evaluate\n",
    "  for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      inputs = {'input_ids':      batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'token_type_ids': batch[2],\n",
    "                'labels':         batch[3]}\n",
    "      outputs = model(**inputs)\n",
    "      logits = outputs[1]\n",
    "\n",
    "    # Get predictions\n",
    "    if preds is None:\n",
    "      preds = logits.detach().cpu().numpy()\n",
    "    else:\n",
    "      preds = numpy.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "  # Get the most probable prediction\n",
    "  preds = numpy.argmax(preds, axis=1)\n",
    "\n",
    "  return preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:16:37.517183400Z",
     "start_time": "2024-05-22T10:16:37.498191600Z"
    }
   },
   "id": "2a9362a68934816f",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "article_filenames = sorted(glob.glob(os.path.join(dev_articles, \"*.txt\")))\n",
    "\n",
    "output_file = 'datasets/TC.labels_pred_no_weights.txt'\n",
    "f = open(output_file, 'w', newline='')\n",
    "writer = csv.writer(f, delimiter='\\t')\n",
    "eval_dataset, eval_dataframe = generate_TC_eval_dataset_from_article(dev_articles, dev_TC_template, tokenizer)\n",
    "predictions = classify_techniques(eval_dataset, model)\n",
    "for i in range(len(predictions)):\n",
    "  writer.writerow([eval_dataframe[\"id\"][i], LABEL_TO_PROP_TECH[predictions[i]], eval_dataframe[\"seq_starts\"][i], eval_dataframe[\"seq_ends\"][i]])\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:50:02.897969300Z",
     "start_time": "2024-05-22T16:50:02.878978500Z"
    }
   },
   "id": "8b265cbb9e69aa99",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### References:\n",
    "The structure of data preporation is the modification of Henry Kim's [implementation](https://medium.com/@jihwangk/fine-grained-propaganda-detection-and-classification-with-bert-dfad4acaa321).\n",
    "\n",
    "The structure of train() function is a modification of Thilina Rajapakse’s [implementation](https://github.com/ThilinaRajapakse/pytorch-transformers-classification)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a74060592a2408d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
