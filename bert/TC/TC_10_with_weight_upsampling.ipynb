{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Technical level - 10 classes with weights with upsampling\n",
    "Model: google-bert/bert-base-multilingual-uncased\n",
    "dev: 20%\n",
    "max_seq_length: 128\n",
    "train_batch_size: 8,\n",
    "eval_batch_size: 8,\n",
    "num_train_epochs: 5\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "242bb28e0578a4a5"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:07.671631600Z",
     "start_time": "2024-05-22T16:47:07.562091900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from tqdm import tqdm, tqdm_notebook, trange"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset, WeightedRandomSampler)\n",
    "from transformers import (BertConfig, AutoTokenizer, BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:08.447496900Z",
     "start_time": "2024-05-22T16:47:08.321986400Z"
    }
   },
   "id": "9e354f6c4cd92628",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "182ba2744f21a010"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# PATH\n",
    "train_articles = \"datasets/upsampling/train\"\n",
    "dev_articles = \"datasets/dev\"\n",
    "train_TC_labels = \"datasets/upsampling/train_TC_labels\"\n",
    "dev_TC_template = \"datasets/TC_labels_for_eval.txt\"  \n",
    "true_file = \"datasets/TC.labels_true.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:10.110240400Z",
     "start_time": "2024-05-22T16:47:10.017045100Z"
    }
   },
   "id": "5951bb2f432926bc",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#techniques = \"datasets/propaganda-techniques-names.txt\"\n",
    "techniques = \"datasets/propaganda-techniques-names.txt\"\n",
    "PROP_TECH_TO_LABEL = {}  #to dictionary\n",
    "LABEL_TO_PROP_TECH = {}     #to list\n",
    "label = 0\n",
    "with open(techniques, \"r\") as f:\n",
    "  for technique in f:\n",
    "    PROP_TECH_TO_LABEL[technique.replace(\"\\n\", \"\")] = int(label)\n",
    "    LABEL_TO_PROP_TECH[int(label)] = technique.replace(\"\\n\", \"\")\n",
    "    label += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:13.390643900Z",
     "start_time": "2024-05-22T16:47:13.364642200Z"
    }
   },
   "id": "1b1cc9cc7cca1fcc",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "n_gpu = torch.cuda.device_count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:14.651559200Z",
     "start_time": "2024-05-22T16:47:14.597522200Z"
    }
   },
   "id": "f7ac9c13fd8fba18",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\"bert\": (BertConfig, BertForSequenceClassification, BertTokenizer)}  \n",
    "args = {\"data_dir\": \"bert/\",  \n",
    "        \"model_type\": \"bert\", \n",
    "        \"model_name\": \"google-bert/bert-base-multilingual-uncased\",\n",
    "        #\"output_dir\": \"output_dir/output_TC_10_with_weight_up\",\n",
    "        \"output_dir\": \"output_dir/output_TC_10_with_weight_up\",\n",
    "        \"max_seq_length\": 128,  \n",
    "        \"train_batch_size\": 8,\n",
    "        \"eval_batch_size\": 8,\n",
    "        \"num_train_epochs\": 5, \n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 4e-5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"save_steps\": 2000,\n",
    "        \"overwrite_output_dir\": False}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:15.362853200Z",
     "start_time": "2024-05-22T16:47:15.319815900Z"
    }
   },
   "id": "a4ffffe8b2ecd8f2",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bece42e74b332b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def articles_to_dataframe(article_folder, label_folder):\n",
    "  \"\"\"\n",
    "  Preprocesses the articles into dataframes with sequences with binary tags\n",
    "  \"\"\"\n",
    "  # First sort the filenames and make sure we have label file for each articles\n",
    "  article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "  label_filenames = sorted(glob.glob(os.path.join(label_folder, \"*.labels.tsv\")))\n",
    "  assert len(article_filenames) == len(label_filenames)\n",
    "\n",
    "  # Initialize sequences\n",
    "  sequences = []\n",
    "\n",
    "  # For each article, do:\n",
    "  for i in range(len(article_filenames)):\n",
    "    # Read in the article\n",
    "    with codecs.open(article_filenames[i], \"r\", encoding=\"utf8\") as f:\n",
    "      article = f.read()\n",
    "\n",
    "    # Read in the label file and store indices for TC task !!!!\n",
    "    with open(label_filenames[i], \"r\") as f:\n",
    "      reader = csv.reader(f, delimiter=\"\\t\")\n",
    "      article_sequences = []\n",
    "      labels_list = []\n",
    "      for row in reader:\n",
    "        article_sequences.append(article[int(row[2]):int(row[3])])\n",
    "        labels_list.append(PROP_TECH_TO_LABEL[row[1]])\n",
    "\n",
    "    sequence = pd.DataFrame(None, range(len(article_sequences)), [\"label\", \"text\"])\n",
    "    sequence[\"label\"] = labels_list\n",
    "    sequence[\"text\"] = article_sequences   \n",
    "\n",
    "    # Add to the sequences\n",
    "    sequences.append(sequence)\n",
    "\n",
    "  # Concatenate all dataframes\n",
    "  dataframe = pd.concat(sequences, ignore_index=True)\n",
    "\n",
    "  return dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:16.667891900Z",
     "start_time": "2024-05-22T16:47:16.651860800Z"
    }
   },
   "id": "aa0b5ec20e2474b6",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_dataframe_to_features(dataframe, max_seq_length, tokenizer):\n",
    "  \"\"\"\n",
    "  Converts dataframe into features dataframe, where each feature will\n",
    "  take form of [CLS] + A + [SEP]\n",
    "  \"\"\"\n",
    "  # Create features\n",
    "  features = pd.DataFrame(None, range(dataframe.shape[0]), \n",
    "                              [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_ids\"])\n",
    "\n",
    "  # For each sequence, do:\n",
    "  for i in range(len(dataframe)):\n",
    "    # Set first and second part of the sequences\n",
    "    tokens = tokenizer.tokenize(dataframe[\"text\"][i])\n",
    "\n",
    "    # If length of the sequence is greater than max sequence length, truncate it\n",
    "    if len(tokens) > max_seq_length - 2:\n",
    "        tokens = tokens[:(max_seq_length - 2)]\n",
    "\n",
    "    # Concatenate the tokens\n",
    "    tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
    "\n",
    "    # Compute the ids\n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    pad_token = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "    input_ids = input_ids + [pad_token] * padding_length\n",
    "    input_mask = input_mask + [0] * padding_length\n",
    "    segment_ids = segment_ids + [0] * padding_length\n",
    "    label_id = dataframe[\"label\"][i]\n",
    "\n",
    "    # Assert to make sure we have same length\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    # Put the data into features dataframe\n",
    "    features[\"input_ids\"][i] = input_ids\n",
    "    features[\"input_mask\"][i] = input_mask\n",
    "    features[\"segment_ids\"][i] = segment_ids\n",
    "    features[\"label_ids\"][i] = int(label_id)    \n",
    "    \n",
    "\n",
    "  return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:17.619911600Z",
     "start_time": "2024-05-22T16:47:17.502928400Z"
    }
   },
   "id": "6e0efefc17e99462",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_training_dataset_from_articles(articles_folders, labels_folders, tokenizer):\n",
    "  \"\"\"\n",
    "  Generates dataset to go into BERT from articles and labels\n",
    "  \"\"\"\n",
    "    \n",
    "  # For each articles and labels folder set, turn them into dataframes\n",
    "  dataframe_list = []\n",
    "  for i in range(len(articles_folders)):\n",
    "    dataframe_list.append(articles_to_dataframe(articles_folders[i], labels_folders[i]))\n",
    "\n",
    "  # Concatenate the dataframes to make a total dataframe\n",
    "  dataframe = pd.concat(dataframe_list, ignore_index=True)\n",
    "\n",
    "  # Process into features dataframe\n",
    "  features = convert_dataframe_to_features(dataframe, args['max_seq_length'], tokenizer) \n",
    "     \n",
    "  # Creating TensorDataset from features\n",
    "  all_input_ids = torch.tensor(features[\"input_ids\"], dtype=torch.long)\n",
    "  all_input_mask = torch.tensor(features[\"input_mask\"], dtype=torch.long)\n",
    "  all_segment_ids = torch.tensor(features[\"segment_ids\"], dtype=torch.long)\n",
    "  all_label_ids = torch.tensor(features[\"label_ids\"], dtype=torch.long)\n",
    "\n",
    "  dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "  \n",
    "  return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:18.401059300Z",
     "start_time": "2024-05-22T16:47:18.260992800Z"
    }
   },
   "id": "4f8387dcea3489f9",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1b86f90a06d6d1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# With weights\n",
    "def train_with_weights (train_dataset, model):\n",
    "    \"\"\"\n",
    "    Trains the model with training dataset\n",
    "    \"\"\"\n",
    "    #Take classes from train datasets\n",
    "    label_list = [label.item() for _, _, _, label in train_dataset]\n",
    "\n",
    "    # Read class weights from file\n",
    "    class_weights = {}\n",
    "    with open(\"datasets/class_weights_up.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            class_name, weight = line.strip().split(\":\")\n",
    "            class_weights[class_name.strip()] = float(weight.strip())\n",
    "\n",
    "    #Convert classes to int\n",
    "    class_weights = {int(key): value for key, value in class_weights.items()}\n",
    "    weights = [class_weights[label] for label in label_list]\n",
    "\n",
    "    # Use weighted sampler\n",
    "    train_sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size']) \n",
    "    \n",
    "    # Compute the total time\n",
    "    t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "    \n",
    "    # Set the grouped parameters for optimizer\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    # Compute warmup steps\n",
    "    warmup_steps = math.ceil(t_total * args['warmup_ratio'])\n",
    "    args['warmup_steps'] = warmup_steps if args['warmup_steps'] == 0 else args['warmup_steps']\n",
    "    \n",
    "    # Initialize optimizer as Adam with constant weight decay and a linear scheduler with warmup\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['warmup_steps'], num_training_steps=t_total)\n",
    "\n",
    "    # Initialize variables for training\n",
    "    global_step = 0\n",
    "    tr_loss = 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "    \n",
    "    # Start training!\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2], \n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "                \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step() \n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
    "                    output_dir = os.path.join(args['output_dir'], 'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:19.636116800Z",
     "start_time": "2024-05-22T16:47:19.582109Z"
    }
   },
   "id": "5703f32dfb8a8528",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=10, bias=True)\n)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args[\"model_type\"]]\n",
    "config = config_class.from_pretrained(args[\"model_name\"], num_labels=len(PROP_TECH_TO_LABEL))\n",
    "tokenizer = tokenizer_class.from_pretrained(args[\"model_name\"])\n",
    "model = model_class(config)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:25.473765Z",
     "start_time": "2024-05-22T16:47:20.374621700Z"
    }
   },
   "id": "709c24dabb588a99",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = generate_training_dataset_from_articles([train_articles], [train_TC_labels], tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T16:47:40.445145800Z",
     "start_time": "2024-05-22T16:47:25.468764500Z"
    }
   },
   "id": "a79a196de9f4a0cc",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ната\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Windows\\Temp\\ipykernel_21104\\3870336446.py:50: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4533702822d54527a9b53bae267602ea"
      },
      "application/json": {
       "n": 0,
       "total": 556,
       "elapsed": 0.003978729248046875,
       "ncols": null,
       "nrows": null,
       "prefix": "Iteration",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 1/5 [39:22<2:37:31, 2362.90s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d4e76cf90904656828da7b51e315ea8"
      },
      "application/json": {
       "n": 0,
       "total": 556,
       "elapsed": 0.01599860191345215,
       "ncols": null,
       "nrows": null,
       "prefix": "Iteration",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 2/5 [1:15:44<1:52:48, 2256.29s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9c72174793144c7b5e12ead09af9a3b"
      },
      "application/json": {
       "n": 0,
       "total": 556,
       "elapsed": 0.015984535217285156,
       "ncols": null,
       "nrows": null,
       "prefix": "Iteration",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 3/5 [1:52:18<1:14:15, 2227.84s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b3d895b7d0f4f9f8422a43f549de46c"
      },
      "application/json": {
       "n": 0,
       "total": 556,
       "elapsed": 0.007002115249633789,
       "ncols": null,
       "nrows": null,
       "prefix": "Iteration",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 4/5 [2:29:21<37:06, 2226.06s/it]  "
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ad5926501284a209b80930bc23c6a37"
      },
      "application/json": {
       "n": 0,
       "total": 556,
       "elapsed": 0.0060040950775146484,
       "ncols": null,
       "nrows": null,
       "prefix": "Iteration",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [3:05:24<00:00, 2224.97s/it]\n"
     ]
    }
   ],
   "source": [
    "global_step, tr_loss = train_with_weights(train_dataset, model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T19:53:05.417826600Z",
     "start_time": "2024-05-22T16:47:40.450142Z"
    }
   },
   "id": "8f51f3a1c2a68913",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(args['output_dir'])\n",
    "tokenizer.save_pretrained(args['output_dir'])\n",
    "torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T19:56:05.277878900Z",
     "start_time": "2024-05-22T19:56:03.874843400Z"
    }
   },
   "id": "1dfc3a0da2c1c207",
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d024aa4ad299f1ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_TC_eval_dataset_from_article(article_folder, indices_file, tokenizer):\n",
    "  \"\"\"\n",
    "  Generates TC dataset to go into BERT from articles and labels\n",
    "  \"\"\"\n",
    "  # First sort the filenames and make sure we have label file for each articles\n",
    "  article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "  articles = {}\n",
    "\n",
    "  # For each article, read them in:\n",
    "  for i in range(len(article_filenames)):\n",
    "    article_id = os.path.basename(article_filenames[i]).split(\".\")[0][7:]\n",
    "    with codecs.open(article_filenames[i], \"r\", encoding=\"utf8\") as f:\n",
    "      articles[article_id] = f.read()\n",
    "\n",
    "  # Read in indices file\n",
    "  with open(indices_file, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    ids_list = []\n",
    "    seq_starts = []\n",
    "    seq_ends = []\n",
    "    article_sequences = []\n",
    "    for row in reader:\n",
    "      ids_list.append(row[0])\n",
    "      seq_starts.append(row[2])\n",
    "      seq_ends.append(row[3])\n",
    "      article_sequences.append(articles[row[0]][int(row[2]):int(row[3])])\n",
    "\n",
    "  dataframe = pd.DataFrame(None, range(len(ids_list)), [\"id\", \"seq_starts\", \"seq_ends\", \"label\", \"text\"])\n",
    "  dataframe[\"id\"] = ids_list\n",
    "  dataframe[\"seq_starts\"] = seq_starts\n",
    "  dataframe[\"seq_ends\"] = seq_ends\n",
    "  dataframe[\"label\"] = [0] * len(ids_list)\n",
    "  dataframe[\"text\"] = article_sequences\n",
    "\n",
    "  # Process into features dataframe\n",
    "  features = convert_dataframe_to_features(dataframe, args['max_seq_length'], tokenizer)\n",
    "      \n",
    "  # Creating TensorDataset from features\n",
    "  all_input_ids = torch.tensor(features[\"input_ids\"], dtype=torch.long)\n",
    "  all_input_mask = torch.tensor(features[\"input_mask\"], dtype=torch.long)\n",
    "  all_segment_ids = torch.tensor(features[\"segment_ids\"], dtype=torch.long)\n",
    "  all_label_ids = torch.tensor(features[\"label_ids\"], dtype=torch.long)\n",
    "\n",
    "  dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "  return dataset, dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T19:56:23.687926300Z",
     "start_time": "2024-05-22T19:56:23.676927800Z"
    }
   },
   "id": "ba2e7fc6aa760b8",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def classify_techniques(eval_dataset, model):\n",
    "  \"\"\"\n",
    "  Classifies a single article dataset and returns article id with indices list\n",
    "  \"\"\"\n",
    "  # Load the eval data and initialize sampler\n",
    "  eval_sampler = SequentialSampler(eval_dataset)\n",
    "  eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "  # Start Classification\n",
    "  preds = None\n",
    "\n",
    "  # For each batch, evaluate\n",
    "  for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      inputs = {'input_ids':      batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'token_type_ids': batch[2],\n",
    "                'labels':         batch[3]}\n",
    "      outputs = model(**inputs)\n",
    "      logits = outputs[1]\n",
    "\n",
    "    # Get predictions\n",
    "    if preds is None:\n",
    "      preds = logits.detach().cpu().numpy()\n",
    "    else:\n",
    "      preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "  # Get the most probable prediction\n",
    "  preds = np.argmax(preds, axis=1)\n",
    "\n",
    "  return preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T19:57:44.086243100Z",
     "start_time": "2024-05-22T19:57:44.074245400Z"
    }
   },
   "id": "2a9362a68934816f",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_21104\\3423670383.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n"
     ]
    },
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/129 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fbcf2aa6b714128aacd4a9ce93561de"
      },
      "application/json": {
       "n": 0,
       "total": 129,
       "elapsed": 0.0039958953857421875,
       "ncols": null,
       "nrows": null,
       "prefix": "Evaluating",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dev with no synthetis data\n",
    "article_filenames = sorted(glob.glob(os.path.join(dev_articles, \"*.txt\")))\n",
    "output_file = 'datasets/TC.labels_pred_10_with_weight_up.txt'\n",
    "f = open(output_file, 'w', newline='')\n",
    "writer = csv.writer(f, delimiter='\\t')\n",
    "eval_dataset, eval_dataframe = generate_TC_eval_dataset_from_article(dev_articles, dev_TC_template, tokenizer)\n",
    "predictions = classify_techniques(eval_dataset, model)\n",
    "for i in range(len(predictions)):\n",
    "  writer.writerow([eval_dataframe[\"id\"][i], LABEL_TO_PROP_TECH[predictions[i]], eval_dataframe[\"seq_starts\"][i], eval_dataframe[\"seq_ends\"][i]])\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T19:58:15.326465600Z",
     "start_time": "2024-05-22T19:57:44.845957200Z"
    }
   },
   "id": "8b265cbb9e69aa99",
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### References:\n",
    "The structure of data preporation is the modification of Henry Kim's [implementation](https://medium.com/@jihwangk/fine-grained-propaganda-detection-and-classification-with-bert-dfad4acaa321).\n",
    "\n",
    "The structure of train() function is a modification of Thilina Rajapakse’s [implementation](https://github.com/ThilinaRajapakse/pytorch-transformers-classification)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a946249d3da2fc5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
